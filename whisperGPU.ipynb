{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ)\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "# whisper.load_model(\"medium\")\n",
    "\n",
    "# Memory summary:\n",
    "print(torch.cuda.memory_summary())\n",
    "\n",
    "# Debug informations\n",
    "print(torch.cuda.get_device_name())  #Print the \n",
    "print(torch.get_default_dtype()) \n",
    "print(torch.cuda.mem_get_info())\n",
    "\n",
    "\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "# Empty the PyTorch cache reserved for the cuda cores:\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_properties(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def transcribeAudio(modelSize, audio):\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available() == True:\n",
    "        print(f\"Cuda device {torch.cuda.get_device_name()} is avaliable. Setting DEVICE to {torch.cuda.get_device_name()}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        #os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "        #torch.cuda.set_per_process_memory_fraction(0.6)\n",
    "        DEVICE = 'cuda'\n",
    "\n",
    "    model = whisper.load_model(modelSize, device = DEVICE)\n",
    "    print(\"Model Loaded\")\n",
    "    \n",
    "    result = model.transcribe(audio)\n",
    "    print(result[\"text\"])\n",
    "\n",
    "transcribeAudio(\"medium\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0 -  Det er en lille test for at teste, hvordan at whisper AI virker.\n",
      "8:0 -  En del er open AI, som fungerer på en bestemt måde, der eventuelt kan hjælpe med transkibering af forskellige reviews og andre line sager.\n",
      "20:0 -  Det er bare en test for at teste, hvordan den eventuelle vil virke, som med Bessar CudaCos på mit 10,5 sepium.\n",
      "29:0 -  Og GIG-pib.\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Data imports/whispertest.json\") as fp:\n",
    "    output=json.load(fp)\n",
    "\n",
    "def format_time(timestamp):\n",
    "    \n",
    "    newstamp = str(timestamp).replace(\".\", \":\")\n",
    "    return newstamp\n",
    "\n",
    "\n",
    "for d in output:\n",
    "    print(format_time(d[\"start\"]) +\" - \"+ d[\"text\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuVenv",
   "language": "python",
   "name": "gpuvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d2bad37c8a051aca7f751a208b304e35dca3742080d56334b03962602323cda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
